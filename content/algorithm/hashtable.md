---
title: '散列表'
date: 2024-07-04T22:03:32+08:00
params:
    math: true
---

**散列表用的是数组支持按照下标随机访问数据的特性**，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。

![img.png](/images/algorithm/algo-hash-1.png)

我们**通过散列函数把元素的键值映射为下标，然后将数据（实际是key-value对）存储在数组中对应下标的位置**。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

## 散列函数
散列函数，顾名思义，它是一个函数。我们可以把它定义成hash(key)，其中 key 表示元素的键值，hash(key) 的值表示经过散列函数计算得到的散列值。

该如何构造散列函数呢？这里有三点散列函数设计的基本要求：

+ 散列函数计算得到的散列值是一个非负整数
+ 如果 key1 = key2，那 hash(key1) == hash(key2)
+ 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)

在真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也**无法完全避免这种散列冲突**。而且，因为数组的存储空间有限，也会加大散列冲突的概率。

## 散列冲突
再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，**开放寻址法**（open addressing）和**链表法**（chaining）。

1. **开放寻址法**
   
开放寻址法的核心思想是，如果出现了散列冲突，我们就**重新探测一个空闲位置**，将其插入。那如何重新探测新的位置呢？我先讲一个比较简单的探测方法，**线性探测**（Linear Probing）。

当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。

下图中黄色的色块表示空闲位置，橙色的色块表示已经存储了数据。

![img.png](/images/algorithm/algo-hash-2.png)

从上图中可以看出，散列表的大小为 10，在元素 x 插入散列表之前，已经 6 个元素插入到散列表中。x 经过 Hash 算法之后，被散列到位置下标为 7 的位置，但是这个位置已经有数据了，所以就产生了冲突。于是我们就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是我们再从表头开始找，直到找到空闲位置 2，于是将其插入到这个位置。

在散列表中查找元素的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。

![img.png](/images/algorithm/algo-hash-3.png)

散列表跟数组一样，不仅支持插入、查找操作，还支持删除操作。对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。我们不能单纯地把要删除的元素设置为空。

在查找的时候，一旦我们通过线性探测方法，**找到一个空闲位置**，我们就可以认定散列表中**不存在**这个数据。但是，**如果这个空闲位置是我们后来删除的**，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。这个问题如何解决呢？

我们可以将删除的元素，**特殊标记为 deleted**。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。

![img.png](/images/algorithm/algo-hash-4.png)

线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。

对于开放寻址冲突解决方法，除了线性探测方法之外，还有另外两种比较经典的探测方法，**二次探测**（Quadratic probing）和**双重散列**（Double hashing）。

所谓二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而**二次探测探测的步长就变成了原来的“二次方”**，也就是说，它探测的下标序列就是 hash(key)+0，$hash(key)+1^2$，$hash(key)+2^2$……

所谓双重散列，意思就是不仅要使用一个散列函数。我们使用**一组散列函数** hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，**如果计算得到的存储位置已经被占用，再用第二个散列函数**，依次类推，直到找到空闲的存储位置。

不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们用**装载因子**（load factor）来表示空位的多少。

```text
散列表的装载因子 = 填入表中的元素个数 / 散列表的长度
```

装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。

2. **链表法**

链表法是一种更加常用的散列冲突解决办法。在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，**所有散列值相同的元素我们都放到相同槽位对应的链表中**。

![img.png](/images/algorithm/algo-hash-5.png)

当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以**插入的时间复杂度是 O(1)**。当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。查找和删除这两个操作的时间复杂度跟链表的长度 k 成正比，也就是 **O(k)**。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。

## 如何设计散列函数
散列表的查询效率并不能笼统地说成是 O(1)。它跟散列函数、装载因子、散列冲突等都有关系。如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降。

**散列函数的设计不能太复杂**。过于复杂的散列函数，势必会消耗很多计算时间，也就间接的影响到散列表的性能。

**散列函数生成的值要尽可能随机并且均匀分布**，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。

## 装载因子过大怎么办
装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。

针对散列表，**当装载因子过大时，我们也可以进行动态扩容**，重新申请一个更大的散列表，将**数据搬移**到这个新散列表中。假设每次扩容我们都申请一个原来散列表大小两倍的空间。如果原来散列表的装载因子是 0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了 0.4。

针对数组的扩容，数据搬移操作比较简单。但是，针对散列表的扩容，数据搬移操作要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。下图中，在原来的散列表中，21 这个元素原来存储在下标为 0 的位置，搬移到新的散列表中，存储在下标为 7 的位置。

![img.png](/images/algorithm/algo-hash-6.png)

插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。用摊还分析法，**均摊情况下，时间复杂度接近最好情况，就是 O(1)**。

实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，**启动动态缩容**。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。

## 如何避免低效扩容
大部分情况下，动态扩容的散列表插入一个数据都很快，但是在特殊情况下，当装载因子已经到达阈值，需要先进行扩容，再插入数据。这个时候，插入数据就会变得很慢，甚至会无法接受。

举一个极端的例子，如果散列表当前大小为 1GB，要想扩容为原来的两倍大小，那就需要对 1GB 的数据重新计算哈希值，并且从原来的散列表搬移到新的散列表，本次插入就十分耗时。

为了解决一次性扩容耗时过多的情况，我们可以将**扩容操作穿插在插入操作的过程中，分批完成**。当装载因子触达阈值之后，我们**只申请新空间**，但并不将老的数据搬移到新散列表中。

当有新数据要插入时，我们将**新数据插入新散列表**中，并且从**老的散列表中拿出一个数据**放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。

![img.png](/images/algorithm/algo-hash-7.png)

这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们**先从新散列表中查找，如果没有找到，再去老的散列表中查找**。

通过这样均摊的方法，将一次性扩容的代价，**均摊**到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，**插入一个数据的时间复杂度都是 O(1)**。

## 如何选择冲突解决方法
1. **开放寻址法**

散列表中的数据都存储在数组中，可以**有效地利用 CPU 缓存**加快查询速度。而且，这种方法实现的散列表，**序列化起来比较简单**。链表法包含指针，序列化起来就没那么容易。

用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，**装载因子的上限不能太大**。这也导致这种方法比链表法**更浪费内存空间**。

**当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。**

2. **链表法**

链表法对**内存的利用率比开放寻址法要高**。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。

链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于 1 的情况。接近 1 时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。

链表因为**要存储指针，所以对于比较小的对象的存储，是比较消耗内存的**，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对 CPU 缓存是不友好的，这方面对于执行效率也有一定的影响。

当然，如果我们存储的是大对象，也就是说要存储的对象的大小**远远大于一个指针的大小**（4 个字节或者 8 个字节），那链表中指针的内存消耗在大对象面前就可以忽略了。

**基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。**

## Java 中 HashMap 分析
1. **初始大小**

HashMap 默认的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高 HashMap 的性能。

2. **装载因子和动态扩容**

最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。

3. **散列冲突解决方法**

HashMap 底层采用**链表法**来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性能。于是，在 JDK1.8 版本中，为了对 HashMap 做进一步优化，引入了**红黑树**。**而当链表长度太长（默认超过 8）时，链表就转换为红黑树**。我们可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。**当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表**。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。

4. **散列函数**

散列函数的设计并不复杂，追求的是简单高效、分布均匀。
```java
int hash(Object key) {
    int h = key.hashCode()；
    return (h ^ (h >>> 16)) & (capitity -1); //capicity表示散列表的大小
}
```

## LRU 缓存淘汰算法
我们需要维护一个**按照访问时间从大到小有序排列**的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将**链表头部的结点删除**。

当要缓存某个数据的时候，先在链表中查找这个数据。**如果没有找到，则直接将数据放到链表的尾部；如果找到了，我们就把它移动到链表的尾部**。因为查找数据需要遍历链表，所以**单纯用链表**实现的 LRU 缓存淘汰算法的时间复杂很高，是**O(n)**。

一个 缓存(cache) 系统主要包含下面几个操作：

+ 往缓存中添加一个数据；
+ 从缓存中删除一个数据；
+ 在缓存中查找一个数据。

这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是 O(n)。如果我们将**散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到 O(1)**。具体的结构就是下面这个样子：

![img.png](/images/algorithm/algo-hash-8.png)

我们使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段 hnext。这个 hnext 有什么作用呢？

因为我们的散列表是通过链表法解决散列冲突的，所以**每个结点会在两条链中**。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。**前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中**。

> Q: 为什么一个结点会在两条链中
> 
> A: hnext 是为了解决哈希冲突的指针，也就是图中**同一行**的顺序链表。但是LRU还需要维护每个节点的访问时间，同一行的结点顺序没有实际意义，只是把哈希冲突的结点串在一起，所以我们需要将结点用前驱后继指针把**不同行的结点**串成另一条时间链。

首先，我们来看如何查找一个数据。我们前面讲过，散列表中查找数据的时间复杂度接近 O(1)，所以通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。

其次，我们来看如何删除一个数据。我们需要找到数据所在的结点，然后将结点删除。借助散列表，我们可以在 O(1) 时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针 O(1) 时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要 O(1) 的时间复杂度。

最后，我们来看如何添加一个数据。添加数据到缓存稍微有点麻烦，我们需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。

## Redis 有序数组
在 redis 中跳表中，实际上，在有序集合中，每个成员对象有两个重要的属性，key（键值）和score（分值）。我们不仅会通过 score 来查找数据，还会通过 key 来查找数据。

举个例子，比如用户积分排行榜有这样一个功能：我们可以通过用户的 ID 来查找积分信息，也可以通过积分区间来查找用户 ID 或者姓名信息。这里包含 ID、姓名和积分的用户信息，就是成员对象，用户 ID 就是 key，积分就是 score。

所以，如果我们细化一下 Redis 有序集合的操作，那就是下面这样：

+ 添加一个成员对象；
+ 按照键值来删除一个成员对象；
+ 按照键值来查找一个成员对象；
+ 按照分值区间查找数据，比如查找积分在 [100, 356] 之间的成员对象；
+ 按照分值从小到大排序成员变量；

如果我们仅仅按照分值将成员对象组织成跳表的结构，那按照键值来删除、查询成员对象就会很慢，解决方法与 LRU 缓存淘汰算法的解决方法类似。我们可以再按照键值构建一个散列表，这样按照 key 来删除、查找一个成员对象的时间复杂度就变成了 O(1)。同时，借助跳表结构，其他操作也非常高效。

## Java LinkedHashMap
我们先来看一段代码。你觉得这段代码会以什么样的顺序打印 3，1，5，2 这几个 key 呢？原因又是什么呢？

```java
HashMap<Integer, Integer> m = new LinkedHashMap<>();
m.put(3, 11);
m.put(1, 12);
m.put(5, 23);
m.put(2, 22);

for (Map.Entry e : m.entrySet()) {
    System.out.println(e.getKey());
}
```

上面的代码会按照数据插入的顺序依次来打印，也就是说，打印的顺序就是 3，1，5，2。你有没有觉得奇怪？散列表中数据是经过散列函数打乱之后无规律存储的，这里是如何实现按照数据的插入顺序来遍历打印的呢？

LinkedHashMap 也是通过散列表和链表组合在一起实现的。实际上，它**不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据**。

```java
HashMap<Integer, Integer> m = new LinkedHashMap<>(10, 0.75f, true);
m.put(3, 11);
m.put(1, 12);
m.put(5, 23);
m.put(2, 22);

m.put(3, 26); // 第 8 行
m.get(5);

for (Map.Entry e : m.entrySet()) {
    System.out.println(e.getKey());
}
```
这段代码打印的结果是1，2，3，5。我来具体分析一下，为什么这段代码会按照这样顺序来打印。

每次调用 put() 函数，往 LinkedHashMap 中添加数据的时候，都会将数据添加到链表的尾部，所以，在前四个操作完成之后，链表中的数据是下面这样：

![img.png](/images/algorithm/algo-hash-9.png)

在第 8 行代码中，再次将键值为 3 的数据放入到 LinkedHashMap 的时候，会先查找这个键值是否已经有了，然后，再将已经存在的 (3,11) 删除，并且将新的 (3,26) 放到链表的尾部。所以，这个时候链表中的数据就是下面这样

![img.png](/images/algorithm/algo-hash-10.png)

当第 9 行代码访问到 key 为 5 的数据的时候，我们将被访问到的数据移动到链表的尾部。所以，第 9 行代码之后，链表中的数据是下面这样：

![img.png](/images/algorithm/algo-hash-11.png)

按照访问时间排序的 LinkedHashMap 本身就是一个支持 LRU 缓存淘汰策略的缓存系统？实际上，它们两个的实现原理也是一模一样的。

**LinkedHashMap 是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap 中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。**





